# -*- coding: utf-8 -*-
"""Task-2-v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T1yA45gO7dweIgTZcTWw-YHgFpAdmOSS

###Task 2: Building a custom Web Scarpper for searching youtube channel links###

####***Author:*** Shirshakk Purkayastha####

#####Script 2: Gathering via 'googlesearch-python' API/Library method#####

#### **Google Search API** allows only *100* requests per query to be displayed. To extend this quota, one needs to enable a subscription of $5 per hour that enables to get an extension of 10k queries per hour.####

---

Installing the required Dependencies
"""

!pip install googlesearch-python

"""Importing Libraries"""

import csv
import time
from googlesearch import search

"""Function to scrape for the google query"""

def fetch_youtube_links(query, max_results):
    youtube_links = []

    # Perform the Google search and retrieve the URLs
    search_results = search(query, num_results=max_results)

    # Iterate through the search results and extract YouTube links
    for url in search_results:
        if 'youtube.com/watch' in url:
            youtube_links.append(url)
        time.sleep(5)

    return youtube_links

"""Function to store results into a '.csv' file"""

def save_to_csv(youtube_links, filename):
    with open(filename, 'w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow(['YTChannel Links'])
        writer.writerows([[link] for link in youtube_links])

"""Function to scrape youtube links from the input search query:"""

def scrape_youtube_links(query, max_results, filename):
    youtube_links = fetch_youtube_links(query, max_results)
    save_to_csv(youtube_links, filename)

"""Main Function"""

if __name__ == "__main__":
    query = "site:youtube.com openinapp.co"
    max_results = 10000
    filename = "YT_Channel_Links.csv"

    scrape_youtube_links(query, max_results, filename)